% $Id: template.tex 11 2007-04-03 22:25:53Z jpeltier $

\documentclass{vgtc}                          % final (conference style)
%\documentclass[review]{vgtc}                 % review
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint]{vgtc}               % preprint
%\documentclass[electronic]{vgtc}             % electronic version

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Figures should be in CMYK or Grey scale format, otherwise, colour 
%% shifting may occur during the printing process.

%% These few lines make a distinction between latex and pdflatex calls and they
%% bring in essential packages for graphics and font handling.
%% Note that due to the \DeclareGraphicsExtensions{} call it is no longer necessary
%% to provide the the path and extension of a graphics file:
%% \includegraphics{diamondrule} is completely sufficient.
%%
\ifpdf%                                % if we use pdflatex
  \pdfoutput=1\relax                   % create PDFs from pdfLaTeX
  \pdfcompresslevel=9                  % PDF Compression
  \pdfoptionpdfminorversion=7          % create PDF 1.7
  \ExecuteOptions{pdftex}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg} % for pdflatex we expect .pdf, .png, or .jpg files
\else%                                 % else we use pure latex
  \ExecuteOptions{dvips}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.eps}     % for pure latex we expect eps files
\fi%

%% it is recomended to use ``\autoref{sec:bla}'' instead of ``Fig.~\ref{sec:bla}''
\graphicspath{{figures/}{pictures/}{images/}{./}} % where to search for the images

\usepackage{microtype}                 % use micro-typography (slightly more compact, better to read)
\PassOptionsToPackage{warn}{textcomp}  % to address font issues with \textrightarrow
\usepackage{textcomp}                  % use better special symbols
\usepackage{mathptmx}                  % use matching math font
\usepackage{times}                     % we use Times as the main font
\renewcommand*\ttdefault{txtt}         % a nicer typewriter font
\usepackage{cite}                      % needed to automatically sort the references
% \usepackage{tabu}                      % only used for the table example
% \usepackage{booktabs}                  % only used for the table example
%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.
% * <jchae@purdue.edu> 2017-07-05T19:53:15.710Z:
%
% ^.

\let\svthefootnote\thefootnote

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
\vgtcinsertpkg

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% Paper title.

\title{Visualization for Classification in Deep Neural Networks}

%% This is how authors are specified in the conference style

%% Author and Affiliation (single author).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}}
%%\affiliation{\scriptsize Allied Widgets Research}

%% Author and Affiliation (multiple authors with single affiliations).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com} %
%%\and Ed Grimley\thanks{e-mail:ed.grimley@aol.com} %
%%\and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}}
%%\affiliation{\scriptsize Martha Stewart Enterprises \\ Microsoft Research}

%% Author and Affiliation (multiple authors with multiple affiliations)
\author{Junghoon Chae\thanks{e-mail: chaej@ornl.gov}\qquad Shang Gao\thanks{e-mail:gaos@ornl.gov}\qquad Arvind Ramanthan\thanks{e-mail:ramanathana@ornl.gov}\qquad Chad Steed\thanks{e-mail:csteed@ornl.gov}\qquad Georgia D. Tourassi\thanks{e-mail:tourassig@ornl.gov} \\ %
        \scriptsize Oak Ridge National Laboratory %
%      \parbox{1.4in}{\scriptsize \centering Martha Stewart Enterprises \\ Microsoft Research}
}


%% A teaser figure can be included as follows, but is not recommended since
%% the space is now taken up by a full width abstract.
\teaser{
 \includegraphics[width=0.98\linewidth]{teaser_v2}
 \caption{A visual analytics tool to understand classification results and suggest potential directions during the development of a Deep Neural Networks model.}
 \label{fig:teaser}
}

%% Abstract section.
\abstract{
% Classification is one of the major tasks in a wide range of data analysis tasks.
Recently, the techniques based on Deep Neural Networks (DNNs) have achieved a great performance in classification tasks in a wide range of applications, such as image recognition and natural language processing.
However, DNN developers face a lot of trial and error during the development process and spend their efforts in developing their network model through analyzing and understanding the classification results.
As such, tools are needed that help the developers not only understand the results, but also suggest the ways to improve their model.
In this paper, we propose a visual analytics tool for visualizing the classification results during the iterative development pipeline of a DNN model.
Our tool enables exploring the classification results from any type of neural network models, identifying misclassified samples, examining the predicted score distributions of samples, and showing how the outcomes progressively change during the training process.
% Our visual analytics approach enables exploring the classification results from any type of neural network models, examining the predicted score distributions for each sample data, and showing how the results change progressively during the training process in order to understand, diagnose, and improve the network models.
% In this paper, however, we focus on the document classification by a Hierarchical Attention Network (HAN) model based on a Recurrent Neural Network (RNN) ~\cite{yang2016hierarchical} as a case study.
} % end of abstract

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
%% The ``\CCScat'' command takes four arguments.

\CCScatlist{ 
  Visualization, classification, deep neural networks
}

% H.5.2 [Information Interfaces and Presenta- tion]: User Interfaces—GUI; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—Information filtering, relevance feedback

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}
\maketitle
%% \section{Introduction} %for journal use above \firstsection{..} instead
Classification is one of the major tasks in a wide range of data analysis tasks.
Recently, the techniques based on Deep Neural Networks (DNNs) have achieved a great performance in classification tasks for a wide range of applications, such as image~\cite{krizhevsky2012imagenet}, video~\cite{karpathy2014large}, text~\cite{kim2014convolutional}, and natural language~\cite{collobert2011natural}.
However, DNN developers face a lot of trial and error to develop a satisfying DNN model.
During the trial and error process, they spend their efforts in developing their network model through analyzing and understanding intermediate experimental results.
As such, tools are needed that help them not only understand the outcomes, but also suggest the ways to improve their model.

In this paper, we propose a visual analytics tool to understand classification results and suggest potential directions during the iterative development pipeline of a DNN model.
Our visual analytics tool allows users to explore the classification results from any type of neural network models, to identify misclassified samples, to examine the predicted score distributions of samples, and to show how the outcomes progressively change during the training process.
Eventually, our visualization helps the users understand, diagnose, and improve DNN models~\cite{liu2017towards}.
Also, our visual design can be applied to a wide range of data classification tasks in deep learning.
In this paper, we focus on the task of classifying clinical pathology reports by DNNs.
This work is in progress now.
So, we focus on introducing the current system and showing preliminary results.

\let\thefootnote\relax\footnote{\tiny{This manuscript has been authored by UT-Battelle, LLC under Contract No.\ DE-AC05-00OR22725 with the U.S.\ Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).}}

\input{relatedwork}
\input{design}
\input{background}
\input{visualization}
\input{conclusion}

\begin{acknowledgments}
This work has been supported in part by the Joint Design of Advanced Computing Solutions for Cancer (JDACS4C) program established by the U.S. Department of Energy (DOE) and the National Cancer Institute (NCI) of the National Institutes of Health. This work was performed under the auspices of the U.S. Department of Energy by Argonne National Laboratory under Contract DE-AC02-06-CH11357, Lawrence Livermore National Laboratory under Contract DE- AC52-07NA27344, Los Alamos National Laboratory under Contract DE-AC5206NA25396, and Oak Ridge National Laboratory under Contract DE-AC05-00OR22725. This research was supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration.
\end{acknowledgments}

%\bibliographystyle{abbrv}
\bibliographystyle{abbrv-doi}
%\bibliographystyle{abbrv-doi-narrow}
%\bibliographystyle{abbrv-doi-hyperref}
%\bibliographystyle{abbrv-doi-hyperref-narrow}

\bibliography{template}

\begin{figure*}[tb]
\centering
\includegraphics[width=1.0\linewidth]{doc-image}
\caption{Detail View: Visualizing important words and sentences in a pathology report.}
\label{fig:detail-view}
%\vspace{-0.8cm}
\end{figure*}


\begin{figure*}[tbh]
\centering
\includegraphics[width=1.0\linewidth]{grid-mode}
\caption{Grid-based classification view. This type can handle more number of samples.}
\label{fig:grid-mode}
%\vspace{-0.8cm}
\end{figure*}

\end{document}
